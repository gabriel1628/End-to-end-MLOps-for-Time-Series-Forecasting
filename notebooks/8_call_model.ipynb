{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import json\n",
    "from preprocessing.preprocessing import *\n",
    "\n",
    "preprocessing_version = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.20.0'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lightgbm\"\n",
    "preprocessing_version = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a terminal, run:\n",
    "```\n",
    "mlflow server --host 127.0.0.1 --port 5000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using preprocessing version 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabriel/Documents/Git/End-to-end MLOps for Time Series/.venv/lib/python3.10/site-packages/mlforecast/core.py:455: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[feat_name] = feat_vals[restore_idxs]\n",
      "/Users/gabriel/Documents/Git/End-to-end MLOps for Time Series/.venv/lib/python3.10/site-packages/mlforecast/core.py:455: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[feat_name] = feat_vals[restore_idxs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (944799, 99)\n",
      "y shape : (944799,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/consumption.csv\")\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "\n",
    "print(f\"Using preprocessing version {preprocessing_version}\")\n",
    "preprocessing = vars()[f\"preprocessing_{preprocessing_version}\"]\n",
    "X, y = preprocessing(df)\n",
    "print(f\"X shape : {X.shape}\")\n",
    "print(f\"y shape : {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model before deployment\n",
    "\n",
    "Run the following code to validate model inference works on the example input data and logged model dependencies, prior to deploying it to a serving endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG: NOT WORKING. Better to switch back to mlflow 2.17\n",
    "\n",
    "# model_uri = 'runs:/963a54cea853490a9b21a465e6e99f94/lightgbm'\n",
    "# # This is the input example logged with the model\n",
    "# pyfunc_model = mlflow.pyfunc.load_model(model_uri)\n",
    "# input_data = pyfunc_model.input_example\n",
    "\n",
    "# # Verify the model with the provided input data using the logged dependencies.\n",
    "# # For more details, refer to:\n",
    "# # https://mlflow.org/docs/latest/models.html#validate-models-before-deployment\n",
    "# mlflow.models.predict(\n",
    "#     model_uri=model_uri,\n",
    "#     input_data=input_data,\n",
    "#     # env_manager=\"uv\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b233aba68d4ffea81131fe5721bf15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.592949529294766, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.592949529294766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([254.6388213])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logged_model = \"runs:/963a54cea853490a9b21a465e6e99f94/lightgbm\"\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "loaded_model.predict(X[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID : 882091402080986748\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment_name = f\"Enefit {model_name} Preprocessing {preprocessing_version}\"\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "print(f\"Experiment ID : {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Run ID: 963a54cea853490a9b21a465e6e99f94\n"
     ]
    }
   ],
   "source": [
    "# Fetch the most recent run\n",
    "runs = client.search_runs([experiment_id], order_by=[\"start_time DESC\"], max_results=10)\n",
    "# runs[0].to_dictionary()[\"data\"][\"metrics\"]#[\"test_mae\"]\n",
    "if runs:\n",
    "    latest_run_id = runs[0].info.run_id\n",
    "    print(f\"Latest Run ID: {latest_run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the below command in a new window. You must be in the same repo as your mlruns directory and have mlflow installed :\n",
      "\n",
      "    mlflow models serve -m ./mlartifacts/882091402080986748/963a54cea853490a9b21a465e6e99f94/artifacts/lightgbm -p 1234\n"
     ]
    }
   ],
   "source": [
    "PORT = 1234\n",
    "print(\n",
    "    f\"\"\"Run the below command in a new window. You must be in the same repo as your mlruns directory and have mlflow installed :\n",
    "\n",
    "    mlflow models serve -m ./mlartifacts/{experiment_id}/{latest_run_id}/artifacts/lightgbm -p {PORT}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "address = \"127.0.0.1\"\n",
    "# address = \"3.252.192.81\"\n",
    "# address = \"ec2-54-228-144-127.eu-west-1.compute.amazonaws.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the model responds\n",
    "url = f\"http://{address}:{PORT}/ping\"\n",
    "r = requests.get(url)\n",
    "print(r)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"dataframe_split\": {\n",
      "        \"columns\": [\n",
      "            \"lag48\",\n",
      "            \"lag49\",\n",
      "            \"lag50\",\n",
      "            \"lag51\",\n",
      "            \"lag52\"\n",
      "        ],\n",
      "        \"data\": [\n",
      "            [\n",
      "                120.54,\n",
      "                134.986,\n",
      "                150.412,\n",
      "                152.763,\n",
      "                136.13\n",
      "            ]\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# See https://mlflow.org/docs/latest/deployment/deploy-model-locally.html#json-input\n",
    "json_data = json.loads(X.iloc[:1, :5].to_json(orient=\"split\"))\n",
    "json_data.pop(\"index\", None)\n",
    "json_data = {\"dataframe_split\": json_data}\n",
    "json_data = json.dumps(json_data, indent=4)\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"dataframe_split\": {\"columns\": [\"lag48\", \"lag49\", \"lag50\", \"lag51\", \"lag52\", \"lag53\", \"lag54\", \"lag55\", \"lag56\", \"lag57\", \"lag58\", \"lag59\", \"lag60\", \"lag61\", \"lag62\", \"lag63\", \"lag64\", \"lag65\", \"lag66\", \"lag67\", \"lag68\", \"lag69\", \"lag70\", \"lag71\", \"lag72\", \"lag73\", \"lag74\", \"lag75\", \"lag76\", \"lag77\", \"lag78\", \"lag79\", \"lag80\", \"lag81\", \"lag82\", \"lag83\", \"lag84\", \"lag85\", \"lag86\", \"lag87\", \"lag88\", \"lag89\", \"lag90\", \"lag91\", \"lag92\", \"lag93\", \"lag94\", \"lag95\", \"expanding_mean_lag48\", \"rolling_mean_lag48_window_size24\", \"expanding_mean_lag49\", \"rolling_mean_lag49_window_size24\", \"expanding_mean_lag50\", \"rolling_mean_lag50_window_size24\", \"expanding_mean_lag51\", \"rolling_mean_lag51_window_size24\", \"expanding_mean_lag52\", \"rolling_mean_lag52_window_size24\", \"expanding_mean_lag53\", \"rolling_mean_lag53_window_size24\", \"expanding_mean_lag54\", \"rolling_mean_lag54_window_size24\", \"expanding_mean_lag55\", \"rolling_mean_lag55_window_size24\", \"expanding_mean_lag56\", \"rolling_mean_lag56_window_size24\", \"expanding_mean_lag57\", \"rolling_mean_lag57_window_size24\", \"expanding_mean_lag58\", \"rolling_mean_lag58_window_size24\", \"expanding_mean_lag59\", \"rolling_mean_lag59_window_size24\", \"expanding_mean_lag60\", \"rolling_mean_lag60_window_size24\", \"expanding_mean_lag61\", \"rolling_mean_lag61_window_size24\", \"expanding_mean_lag62\", \"rolling_mean_lag62_window_size24\", \"expanding_mean_lag63\", \"rolling_mean_lag63_window_size24\", \"expanding_mean_lag64\", \"rolling_mean_lag64_window_size24\", \"expanding_mean_lag65\", \"rolling_mean_lag65_window_size24\", \"expanding_mean_lag66\", \"rolling_mean_lag66_window_size24\", \"expanding_mean_lag67\", \"rolling_mean_lag67_window_size24\", \"expanding_mean_lag68\", \"rolling_mean_lag68_window_size24\", \"expanding_mean_lag69\", \"rolling_mean_lag69_window_size24\", \"expanding_mean_lag70\", \"rolling_mean_lag70_window_size24\", \"expanding_mean_lag71\", \"rolling_mean_lag71_window_size24\", \"month\", \"dayofweek\", \"hour\"], \"data\": [[120.54, 134.986, 150.412, 152.763, 136.13, 121.033, 80.621, 43.428, 46.84, 43.671, 28.354, 24.35, 28.651, 33.456, 43.94, 73.104, 94.903, 111.971, 105.432, 105.858, 103.433, 104.854, 97.033, 109.366, 127.531, 129.917, 157.872, 143.479, 109.22, 72.92, 58.293, 39.982, 49.021, 45.512, 37.784, 26.138, 31.147, 36.071, 54.211, 77.308, 94.592, 96.481, 89.781, 88.184, 87.955, 91.594, 77.691, 96.59, 83.6333958333, 87.2970416667, 82.8481489362, 87.5883333333, 81.7147173913, 87.377125, 80.1881111111, 87.6879583333, 78.5386818182, 87.301125, 77.1993488372, 86.179875, 76.1556904762, 84.1751666667, 76.0467804878, 83.2448333333, 76.86225, 83.10125, 77.6320512821, 83.192125, 78.5257631579, 83.2688333333, 79.8817567568, 83.66175, 81.4243055556, 83.73625, 82.9321142857, 83.84025, 84.3872941176, 83.9492083333, 85.612969697, 84.3771666667, 86.003875, 84.5523333333, 85.7168064516, 84.539375, 84.8416666667, 83.8939583333, 84.1316551724, 83.2418333333, 83.3557142857, 82.5054166667, 82.6121111111, 81.8605, 81.7566538462, 81.308, 81.1456, 80.5020833333, 9, 5, 23]]}}'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = json.loads(X.iloc[:1, :].to_json(orient=\"split\"))\n",
    "json_data.pop(\"index\", None)\n",
    "json_data = {\"dataframe_split\": json_data}\n",
    "json.dumps(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get predictions\n",
    "url = f\"http://{address}:{PORT}/invocations\"\n",
    "r = requests.post(url, headers={\"Content-Type\": \"application/json\"}, json=json_data)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
